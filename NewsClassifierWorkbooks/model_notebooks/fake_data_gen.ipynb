{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = ['jumped', 'hike', 'good','rise','rose', 'growth', 'falling interest rates',\\\n",
    "             'bull','truce','bullish','bull','bullish','bulls','bullish','bull','bullish', 'great',\\\n",
    "             'optimistic', 'rally', 'surge', 'loved', 'dividend increases', 'benifit'\\\n",
    "             'soared', 'growth' 'loss','buy', 'higher','break resistance','gain','better', 'gains', 'outperform',\\\n",
    "             'well', 'solid', 'regret not buying','keep buying', 'best', 'strong'\\\n",
    "             'hot', 'love','hottest', 'best', 'beat analyst expectations','up', 'frontrunner', 'new funding',\n",
    "             'fresh funding', 'acquire', 'high','higher','highest','high'\\\n",
    "            ]\n",
    "neg_words = ['bearish', 'bears','bad','poor','expansion','slow', 'bear','bearish', 'bears', 'bear','bearish', 'bears', 'bear',\\\n",
    "             'bear','bearish','underperform', 'rising interest rates', 'lower','declared unsafe',\\\n",
    "             'caught','cheap for a reason','caution','cautions','criticized','decline', 'toll' \\\n",
    "             'slumped','market drop','regret not selling','brace','pressure', 'fell','terrible','awful', 'profit','worry','fell below support',\\\n",
    "             'bearish', 'miss','shock','fell short of analyst expectations','fizzle','pullback'\\\n",
    "             'sell', 'losses','fail','not buying','muted', 'slide','warn','worst', 'rattle','stale',\\\n",
    "             'plummet', 'down', 'low', 'disappointed', 'weak', 'slow', 'growth', 'stalled growth',\\\n",
    "             'worry', 'low', 'warning','struggle','retreat', 'mute', 'fear', 'downside']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 67\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_words), len(neg_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('20k.txt','r') as f:\n",
    "    common_words = f.read()\n",
    "common_words = common_words.split('\\n')[:20_000]\n",
    "hyper_common_words, common_words = common_words[:100],common_words[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_words = \"\"\"Actively Managed Funds\n",
    "Fibonacci retracement\n",
    "Pennant\n",
    "sentiment\n",
    "flag\n",
    "wedge\n",
    "Bollinger Bands\n",
    "sma\n",
    "simple moving average\n",
    "donald trump\n",
    "boss\n",
    "corporate\n",
    "bussiness\n",
    "politics\n",
    "president\n",
    "alpha\n",
    "beta\n",
    "price\n",
    "bond\n",
    "blue chip\n",
    "investing\n",
    "Business Inventories\n",
    "buy side \n",
    "asset\n",
    "capital\n",
    "Capital Expenditure\n",
    "Collateralized Debt Obligation\n",
    "Common Stock\n",
    "Consumer Price Index\n",
    "Convertible Securities\n",
    "Convexity\n",
    "Corporate Bonds\n",
    "Covered Call\n",
    "Credit Default Swap\n",
    "Credit Risk\n",
    "Credit Spread\n",
    "Crude Oil\n",
    "bitcoin\n",
    "gold\n",
    "silver\n",
    "yen\n",
    "dollar\n",
    "earning\n",
    "Dow Jones Industrial Average\n",
    "dow\n",
    "Dogs of the Dow\n",
    "EBITDA\n",
    "Economic Indicators\n",
    "Federal Reserve\n",
    "Federal Open Market Committee\n",
    "Federal Funds Rate\n",
    "Float\n",
    "Fundamentals\n",
    "Gross Domestic Product\n",
    "Growth Funds\n",
    "Head and Shoulders\n",
    "Hedge Fund\n",
    "Housing Starts\n",
    "International Monetary Fund\n",
    "Initial Jobless Claims\n",
    "International Funds\n",
    "Interest Rate\n",
    "cut\n",
    "take\n",
    "Intrinsic Value\n",
    "Investment Bank\n",
    "Moving Average\n",
    "Municipal Bonds\n",
    "Put\n",
    "Call\n",
    "earning call\n",
    "Passively Managed Funds\n",
    "PEG Ratio\n",
    "Preferred Stock\n",
    "Price Earnings\n",
    "option\n",
    "S&P 500\n",
    "Short Selling\n",
    "Spread\n",
    "Spread Product\n",
    "Standard Deviation\n",
    "Support\n",
    "Swaps\n",
    "Treasury Bills\n",
    "Treasury Futures\n",
    "Treasury Notes\n",
    "Treasury Securities\n",
    "Treasury Bond\n",
    "Value Stock\n",
    "VIX\n",
    "Volatility\n",
    "When Issued Trading\n",
    "Whisper Number\n",
    "Yield Curve\"\"\".split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['ANTM', 'Anthem Inc.', 'Health Care'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "with open('companies.csv','r') as f:\n",
    "    companies = pd.read_csv(f).as_matrix()\n",
    "companies[np.random.randint(0,504)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sma because online Business Inventories view ALLE confusion at underperform Call Standard Deviation are built home seventeen fingering information us Consumer Price Index site fizzle Industrials Credit Spread initial use Bollinger Bands proc bearish been Passively Managed Funds stale poor bear out blue chip Passively Managed Funds link brace criticized decline also view information warn Value Stock Support bussiness bear oldies Common Stock S&P 500 S&P 500 ten Capital Expenditure Allegion',\n",
       " 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pos_words\n",
    "#neg_words\n",
    "#common_words\n",
    "#finance_words\n",
    "#companies\n",
    "def gen_sentence(company):\n",
    "    opinion = np.random.choice([0,1,2], p=[0.4,0.2,0.4])\n",
    "    if opinion==1:\n",
    "        corpus_set = [common_words, hyper_common_words, finance_words, company]\n",
    "        p_set = [0.4,0.3,0.2,0.1]\n",
    "    else:\n",
    "        sentiment_set = [neg_words,'', pos_words][opinion]\n",
    "        corpus_set = [sentiment_set, common_words, hyper_common_words, finance_words, company]\n",
    "        p_set = [0.25,0.25,0.25,0.2,0.05]\n",
    "    sen = np.random.permutation([company[0], company[1], company[2]]).tolist()\n",
    "    for i in range(0, np.random.randint(10,80)):\n",
    "        word_set = np.random.choice(corpus_set, p=p_set)\n",
    "        if  word_set is common_words:\n",
    "            common_word_distrobution = \\\n",
    "            np.arange(len(common_words), 0,-1)/sum(np.arange(len(common_words), 0,-1))\n",
    "            word = np.random.choice(common_words, p=common_word_distrobution/sum(common_word_distrobution))\n",
    "        else:\n",
    "            word = np.random.choice(word_set)\n",
    "        sen.insert(np.random.randint(0,len(sen)), word)\n",
    "    return ' '.join(sen), opinion\n",
    "gen_sentence(companies[22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = '../../dependencies/labeled_data/meta'\n",
    "import pickle \n",
    "with open(data_path, 'rb') as f:\n",
    "    data_raw = pickle.load(f)\n",
    "X,y = np.array(data_raw['X']), np.array(data_raw['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class filter_to_fin():\n",
    "    def __init__(self, model_path:str = '../models/fin_not_fin_v2.model', inverse_recall_rate:float = 0.4):\n",
    "        self.model_path = model_path\n",
    "        self.inverse_recall_rate = inverse_recall_rate\n",
    "    def transform(self, X, y=None):\n",
    "        with open(self.model_path, 'rb') as f:\n",
    "            m_t = pickle.load(f)\n",
    "        #we want a pretty lose recall here, as most of the things coming will be financial news\n",
    "        indexes = m_t['model'].predict_proba(m_t['transformer'].transform(X))[:,1]>self.inverse_recall_rate\n",
    "        return X[indexes], y[indexes]\n",
    "from sklearn.pipeline import TransformerMixin, Pipeline\n",
    "from sklearn.base import  BaseEstimator\n",
    "import urlextract\n",
    "import re\n",
    "class Cleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, include_subj=True, replace_html=True, remove_punctuation=True, replace_urls=True, replace_numbers=True):\n",
    "        self.include_subj = include_subj; self.replace_html = replace_html\n",
    "        self.remove_punctuation = remove_punctuation; self.replace_urls = replace_urls;\n",
    "        self.replace_numbers = replace_numbers  \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def __html_to_plain_text__(self, html: str) -> str:\n",
    "        from bs4 import BeautifulSoup\n",
    "        return BeautifulSoup(html, 'html.parser').get_text()\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for article in X:\n",
    "            text = \" \".join(article) if self.include_subj else \" \".join(article[1:])\n",
    "            if self.replace_html:\n",
    "                text = self.__html_to_plain_text__(text)\n",
    "            if self.replace_urls:\n",
    "                url_extractor = urlextract.URLExtract() \n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" URL \")\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', ' NUMBER ', text)\n",
    "            if self.remove_punctuation:\n",
    "                text = text.replace(\"\\'\", \"\").replace(\"â€™\", \"\") #Because we dont want these to be replaced by spaces\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            X_transformed.append(text)\n",
    "        return X_transformed\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "class CountVectorizerWithStemming(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        lemm = WordNetLemmatizer()\n",
    "        analyzer = super(CountVectorizerWithStemming, self).build_analyzer()\n",
    "        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.18 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.18 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator Pipeline from version 0.18 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.18 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#using the fin_not_fin model to filter out non-financial news\n",
    "X_filtered, y_filtered = filter_to_fin(inverse_recall_rate=0.1).transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1160, 1160)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_filtered), len(y_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_set_x = []\n",
    "gen_set_y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5_000):\n",
    "    sentence, sentiment = gen_sentence(companies[np.random.randint(0,len(companies))])\n",
    "    gen_set_x.append(['artifical gen','','',sentence])\n",
    "    gen_set_y.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1160 5000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_filtered), len(gen_set_x))\n",
    "data_raw = {'X':X_filtered.tolist()+gen_set_x,'y':y_filtered.tolist()+gen_set_y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6160, 6160)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_raw['y']), len(data_raw['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_extended', 'wb') as fp:\n",
    "            pickle.dump(data_raw, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd Abdsd '"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Abdsd \"*80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'downsid'"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import *\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('downside')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/DavidGold/Desktop/SchoolWork/GitHubProjects/summer2k19/datascience/NewsClassifier/NewsClassifierWorkbooks/model_notebooks'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

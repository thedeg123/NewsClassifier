{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1242\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "PATH = 'dependencies/meta'\n",
    "with open(PATH, 'rb') as fp:\n",
    "    data_raw = pickle.load(fp)\n",
    "X, y = data_raw[\"X\"], data_raw[\"y\"]\n",
    "print(len(y))\n",
    "#X: Source, Title, Headline, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('adversarial', 'narrative'),\n",
       " ('asx', 'coh'),\n",
       " ('bme', 'vid'),\n",
       " ('changi', 'airport'),\n",
       " ('conventional', 'wisdom'),\n",
       " ('ed', 'yardeni'),\n",
       " ('hunt', 'showdown'),\n",
       " ('leons', 'furniture'),\n",
       " ('mary', 'altaffer'),\n",
       " ('nintendos', 'lawyers'),\n",
       " ('proshares', 'ultrashort'),\n",
       " ('royal', 'caribbean'),\n",
       " ('sophie', 'turner'),\n",
       " ('susa', 'ventures'),\n",
       " ('tse', 'lnf'),\n",
       " ('venkat', 'subramaniam'),\n",
       " ('wells', 'fargo'),\n",
       " ('angela', 'merkel'),\n",
       " ('crosstown', 'brooklyn'),\n",
       " ('declining', 'num')]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "plain_text = Cleaner().fit_transform([[' '.join([' '.join(i) for i in X])]])[0].split()\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(plain_text)\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder.apply_freq_filter(3)\n",
    "finder.nbest(bigram_measures.pmi, 20)\n",
    "# tokens = nltk.wordpunct_tokenize(text)\n",
    "# finder = BigramCollocationFinder.from_words(tokens)\n",
    "# finder\n",
    "# trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "# finder = BigramCollocationFinder.from_words('a')\n",
    "# print(type(nltk.corpus.genesis.words('english-web.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from html import unescape\n",
    "import string\n",
    "def html_to_plain_text(html: str) -> str:\n",
    "    from bs4 import BeautifulSoup\n",
    "    return BeautifulSoup(html, 'html.parser').get_text()\n",
    "def drop(s):\n",
    "    return re.sub(r'\\W+', ' ', s, flags=re.M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ap mary altaffer nike is scheduled to release fourth quarter earnings after the bell on thursday wall street is maintaining its overwhelmingly bullish view on the shoe giant amid investors fears around business in china where the broader e\n"
     ]
    }
   ],
   "source": [
    "s = html_to_plain_text(X[1][3])\n",
    "s = drop(s)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.pipeline import BaseEstimator, TransformerMixin, Pipeline\n",
    "import urlextract \n",
    "class Cleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, include_subj=True, lower_case=True, remove_punctuation=True,\n",
    "                 replace_urls=True, replace_numbers=True):\n",
    "        self.include_subj = include_subj\n",
    "        self.lower_case = lower_case\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_transformed = []\n",
    "        for article in X:\n",
    "            text = \" \".join(article) if self.include_subj else \" \".join(article[1:3])\n",
    "            text = html_to_plain_text(text)\n",
    "            if self.lower_case:\n",
    "                text = text.lower()\n",
    "            if self.replace_urls:\n",
    "                url_extractor = urlextract.URLExtract() \n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" URL \")\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', ' NUMBER ', text)\n",
    "            if self.remove_punctuation:\n",
    "                text = text.replace(\"\\'\", \"\")\n",
    "                text = text.replace(\"â€™\", \"\")\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            X_transformed.append(text)\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class CleanWordToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, num_poly_features:int = 1, stemming:bool = True):\n",
    "        self.num_poly_features = num_poly_features\n",
    "        self.stemming = stemming\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def __push_pop__(self, s:str, span:int = 1, stemming:bool = True) -> [str]:\n",
    "        poly_arr = s.split()\n",
    "        stemmer = nltk.PorterStemmer()\n",
    "        if stemming:\n",
    "            poly_arr = [stemmer.stem(word) for word in poly_arr]\n",
    "        p2 = span\n",
    "        ret = []\n",
    "        while p2<=len(poly_arr):\n",
    "            ret.append(' '.join(poly_arr[p2-span:p2]))\n",
    "            p2+=1\n",
    "        return ret\n",
    "    def __poly_features_str__(self, s:str, n_splits:int = 1, stemming:bool = True) -> [str]:\n",
    "        '''\n",
    "        instead of splitting a str of \"this is a string\" into [\"this\",\"is\",\"a\", \"string\"] we can\n",
    "        manufacture additional features ex if n_splits=2 it becomes \n",
    "        [\"this\", \"is\", \"a\", \"string\", \"this is\", \"is a\", \"a string\"] \n",
    "        '''\n",
    "        ret = []\n",
    "        for i in range(1,n_splits+1):\n",
    "            ret += self.__push_pop__(s=s, span=i, stemming=stemming)\n",
    "        return ret\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed=[]\n",
    "        for article in X:\n",
    "            word_counts = Counter(\\\n",
    "                        self.__poly_features_str__(\\\n",
    "                        article, n_splits = self.num_poly_features, stemming = self.stemming))\n",
    "            X_transformed.append(word_counts)\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocab_size=1000):\n",
    "        self.vocab_size = vocab_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for (word,count) in word_count.items():\n",
    "                total_count[word]+= min(count,10)\n",
    "        most_common = total_count.most_common()[:self.vocab_size]\n",
    "        self.most_common_ = most_common\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self \n",
    "    def transform(self, X, y=None):\n",
    "        rows,cols,data = [],[],[]\n",
    "        for (row,word_count) in enumerate(X):\n",
    "            for (word,count) in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocab_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_prior_set(X, y, prior_strength:int=1_000) -> bool:\n",
    "    #fix the prior so it just appends the counter and returns\n",
    "    pos_set = ' '.join(['jumped', 'hike', 'rise', 'growth', 'bullish', 'optimistic', 'rally', 'surge', 'soared', 'buy', 'higher', 'gains', 'outperform','lower'])\n",
    "    neg_set = ' '.join(['slumped', 'fell', 'worry', 'bearish', 'miss', 'sell', 'losses', 'warn', 'plummet', 'bad', 'down'])\n",
    "    pos_weight, neg_weight = int(prior_strength * (1-(len(pos_set)/(len(pos_set) + len(neg_set))))), \\\n",
    "                            int(prior_strength * (1-(len(neg_set)/ (len(pos_set) + len(neg_set)))))    \n",
    "    X+=([[\"\",\"\",\"\", pos_set]]*pos_weight)\n",
    "    y+=([2]*pos_weight) #2 is good\n",
    "    X+=([[\"\",\"\",\"\", neg_set]]*neg_weight)\n",
    "    y+=([0]*neg_weight) #0 is bad\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_bayesian, y_bayesian = bayesian_prior_set(X_train,y_train, prior_strength=500)\n",
    "data_pipeline = Pipeline([\n",
    "    (\"text_cleaner\", Cleaner()),\n",
    "    (\"text_to_count\", CleanWordToWordCounterTransformer(stemming=False)),\n",
    "    (\"word_cout_to_vect\", WordCounterToVectorTransformer(10_000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = data_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(577, 10001) (577,)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_train_dec = y_train!=1\n",
    "y_train_yn = y_train[y_train_dec]\n",
    "X_train_transformed_yn = X_train_transformed[y_train_dec]\n",
    "print(X_train_transformed_yn.shape, y_train_yn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV as gscv\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42, n_jobs=-1)\n",
    "param_grid = [\n",
    "    {'max_leaf_nodes':[None],'n_estimators':[10]},\n",
    "  ]\n",
    "grid_search = gscv(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring = \"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acheaved score of:\t 0.7712078639249641\n",
      "With following paramaters:\t {'max_leaf_nodes': None, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "grid_search.fit(X_train_transformed, y_important)\n",
    "print(\"Acheaved score of:\\t\", grid_search.best_score_)\n",
    "print(\"With following paramaters:\\t\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42, n_jobs=-1)\n",
    "#lin_svc = LinearSVC(random_state=42)\n",
    "lin_svc = SVC(kernel='linear',probability=True)\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "#extra_clf = ExtraTreesClassifier(n_estimators=100, max_leaf_nodes=16, n_jobs=-1) #random thresholds set\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators = [('log_reg', log_clf), ('sgd_clf', lin_svc)],\n",
    "    voting = 'hard',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.745763 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.830508 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.810345 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.706897 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.862069 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.824561 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.701754 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.807018 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.824561 -   0.0s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................................... , score=0.754386 -   0.0s\n",
      "LogisticRegression 0.7867862233022651\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.745763 -   0.0s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................................... , score=0.830508 -   0.0s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    1.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................................... , score=0.810345 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.706897 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.844828 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.842105 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.701754 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.824561 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.807018 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.736842 -   0.0s\n",
      "SVC 0.7850620853712305\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    5.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................................... , score=0.677966 -   0.1s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................................... , score=0.677966 -   0.1s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................................... , score=0.689655 -   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.706897 -   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.655172 -   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.666667 -   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.701754 -   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.631579 -   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.649123 -   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.649123 -   0.1s\n",
      "ExtraTreesClassifier 0.6705901955355952\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................................... , score=0.745763 -   0.0s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................................... , score=0.847458 -   0.0s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    1.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................................... , score=0.810345 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.706897 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.862069 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.824561 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.684211 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.807018 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.824561 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.754386 -   0.0s\n",
      "VotingClassifier 0.78672675259159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    6.1s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from warnings import filterwarnings as warn\n",
    "warn(\"ignore\", category=DeprecationWarning)\n",
    "for clf in (log_clf, lin_svc, voting_clf):\n",
    "    score = cross_val_score(clf, X_train_transformed_yn , y_train_yn, cv=10, verbose=3, scoring=\"accuracy\")\n",
    "    print(clf.__class__.__name__, score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "X_test_transformed = data_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63, 10001) (63,)\n"
     ]
    }
   ],
   "source": [
    "y_test = np.array(y_test)\n",
    "y_test_dec = y_test!=1\n",
    "y_test_yn = y_test[y_test_dec]\n",
    "X_test_transformed_yn = X_test_transformed[y_test_dec]\n",
    "print(X_test_transformed_yn.shape, y_test_yn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('log_reg', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)), ('sgd_clf'...',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         n_jobs=1, voting='soft', weights=None)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.fit(X_train_transformed_yn, y_train_yn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = voting_clf.predict(X_test_transformed_yn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8412698412698413"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred, y_test_yn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper = 0.5\n",
    "lower =1-upper\n",
    "pred_bin = pred_num.copy()\n",
    "pred_bin[pred_bin[:,1]>=upper] = 2\n",
    "pred_bin[pred_bin[:,1]<=lower] = 0\n",
    "pred_bin[((pred_bin[:,1] >lower) & (pred_bin[:,1] < upper))] = 1\n",
    "pred_bin =pred_bin[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7117296222664016"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred_bin[:503], y_pred=y_bayesian[:503])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business Insider',\n",
       " \"nike analysts are brushing off china growth fears ahead of the shoe giant's quarterly numbers (nke)\",\n",
       " \"nike is scheduled to release fourth-quarter earnings after the bell on thursday. wall street is maintaining its overwhelmingly bullish view on the shoe giant amid investors' fears around business in china, where the broader economy is slowing. a declining numâ€¦\",\n",
       " \"ap/mary altaffer\\r\\n<ul><li>nike is scheduled to release fourth-quarter earnings after the bell on thursday.</li><li>wall street is maintaining its overwhelmingly bullish view on the shoe giant amid investors' fears around business in china, where the broader e\"]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = [['v','a','so so', '']]\n",
    "pred = data_pipeline.transform(ex).toarray()\n",
    "voting_clf.predict(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Seekingalpha.com',\n",
       " 'gold weekly: rally set to continue',\n",
       " 'gldm has overshot our june target, reaching a high of $14.34 per share this week. speculators are the most bullish on gold since november 2017, having lifted th',\n",
       " \"introduction\\r\\nwelcome to orchid's gold weekly report. we discuss gold prices through the lenses of the world gold shares spdr gold minishares trust etf (gldm) because we think that is the best pure-play etf to assert exposure to spot gold prices.\\r\\ngldm has ov\"]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

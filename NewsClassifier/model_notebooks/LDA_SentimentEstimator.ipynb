{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An entirely unsuccessful attempt at using a semi-supervised LDA for sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from sys import stdout\n",
    "data_path = './datasets/fin_not_fin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    }
   ],
   "source": [
    "def sweep_dir(folder, recursive=True, max_size=30_000):\n",
    "    '''\n",
    "    sweep a given folder for all images, process and load them into a np dataset which gets returned\n",
    "    '''\n",
    "    dataset = []\n",
    "    for file in os.listdir(folder):\n",
    "        if len(dataset) >= max_size:\n",
    "            return dataset\n",
    "        if file.endswith(\".json\"):\n",
    "            with open(''.join([folder,file]), 'r') as fp:  \n",
    "                dataset.append(json.load(fp))\n",
    "        elif os.path.isdir(folder+file) and recursive is True: #if this is a directory\n",
    "            dataset+= sweep_dir(folder+file+'/')\n",
    "    return dataset\n",
    "raw = sweep_dir(data_path+'financial/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import BaseEstimator, TransformerMixin, Pipeline\n",
    "import urlextract\n",
    "import re\n",
    "class Cleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, include_subj=True, replace_html=False, remove_punctuation=True, replace_urls=False, replace_numbers=True):\n",
    "        self.include_subj = include_subj; self.replace_html = replace_html\n",
    "        self.remove_punctuation = remove_punctuation; self.replace_urls = replace_urls;\n",
    "        self.replace_numbers = replace_numbers\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_transformed = []\n",
    "        for article in X:\n",
    "            if type(article) is dict:\n",
    "                text = \" \".join([article['title'], article['text']]) if self.include_subj else article['text']\n",
    "            elif type(article) is list:\n",
    "                text = \" \".join(article)\n",
    "            else:\n",
    "                text = article\n",
    "            if self.replace_html:\n",
    "                text = html_to_plain_text(text)\n",
    "            if self.replace_urls:\n",
    "                url_extractor = urlextract.URLExtract() \n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" URL \")\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', ' NUMBER ', text)\n",
    "            if self.remove_punctuation:\n",
    "                text = text.replace(\"\\'\", \"\")\n",
    "                text = text.replace(\"’\", \"\")\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            X_transformed.append(text)\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "class CountVectorizerWithStemming(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        lemm = WordNetLemmatizer()\n",
    "        analyzer = super(CountVectorizerWithStemming, self).build_analyzer()\n",
    "        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x20000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 166844 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "cvws = CountVectorizerWithStemming(stop_words=\"english\", max_features=20_000)\n",
    "text_clf = Pipeline([\n",
    "    ('clean', Cleaner()), #cleans text\n",
    "    ('vect', cvws), #turns words to counts \n",
    "    #('tfidf', TfidfTransformer()), #turns counts to tf-idf\n",
    "])\n",
    "X_train_prepared = text_clf.fit_transform(X[:1000])\n",
    "X_train_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeding guided LDA\n",
    "LDASeedList = [ ['jumped', 'hike', 'rise', 'growth', 'bullish', 'optimistic', 'rally', 'surge', \\\n",
    "                'soared', 'growth' 'buy', 'higher', 'gains', 'outperform','lower'],\n",
    "                ['slumped', 'fell', 'worry', 'bearish', 'miss', 'sell', 'losses', 'warn', \\\n",
    "                 'plummet', 'bad', 'down', 'low', 'disappointed', 'weak', 'worry']\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x20000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 166844 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_loc = {word:i for i,word in enumerate(cvws.get_feature_names())}\n",
    "LDASeedList_prepared = {}\n",
    "for t_id, st in enumerate(LDASeedList):\n",
    "    for word in st:\n",
    "        try:\n",
    "            LDASeedList_prepared[key_loc[word]] = t_id\n",
    "        except KeyError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 1000\n",
      "INFO:guidedlda:vocab_size: 20000\n",
      "INFO:guidedlda:n_words: 291803\n",
      "INFO:guidedlda:n_topics: 2\n",
      "INFO:guidedlda:n_iter: 100\n",
      "/anaconda3/lib/python3.6/site-packages/guidedlda/utils.py:55: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if sparse and not np.issubdtype(doc_word.dtype, int):\n",
      "INFO:guidedlda:<0> log likelihood: -2630843\n",
      "INFO:guidedlda:<20> log likelihood: -2436418\n",
      "INFO:guidedlda:<40> log likelihood: -2410220\n",
      "INFO:guidedlda:<60> log likelihood: -2403653\n",
      "INFO:guidedlda:<80> log likelihood: -2400710\n",
      "INFO:guidedlda:<99> log likelihood: -2398679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<guidedlda.guidedlda.GuidedLDA at 0x1a34021c18>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import guidedlda as LDA\n",
    "#fix the hyperparams to be in the correct format and well be in business. Check CountVectorizer\n",
    "#documentation to see how words in \n",
    "model = LDA.GuidedLDA(n_topics=2, n_iter=100, random_state=42, refresh=20)\n",
    "model.fit(X_train_prepared, seed_topics= LDASeedList_prepared, seed_confidence=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: number, year, market, said, company, percent, bank, cent, share, price \n",
      "Topic 1: said, government, business, financial, fund, say, make, new, tax, people \n"
     ]
    }
   ],
   "source": [
    "n_top_words = 10\n",
    "for i,topic_dist in enumerate(model.topic_word_):\n",
    "    topic_words = np.array(cvws.get_feature_names())[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print(\"Topic {}: {} \".format(i,', '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.491167385522915, 0.20612463500845246]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = 8\n",
    "LDAPredict(X_train_prepared[val], model.topic_word_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-cc2b5b912287>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_word_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_loc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'good'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "model.topic_word_[3][key_loc['good']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'organizations': [],\n",
       " 'uuid': 'cb34de284d3c1c2434da1c3fe59e85e692f2fbfb',\n",
       " 'thread': {'social': {'gplus': {'shares': 0},\n",
       "   'pinterest': {'shares': 0},\n",
       "   'vk': {'shares': 0},\n",
       "   'linkedin': {'shares': 0},\n",
       "   'facebook': {'likes': 0, 'shares': 0, 'comments': 0},\n",
       "   'stumbledupon': {'shares': 0}},\n",
       "  'site_full': 'business-news.thestreet.com',\n",
       "  'main_image': '',\n",
       "  'site_section': 'http://business-news.thestreet.com/ile-camera/rss/627',\n",
       "  'section_title': 'Ile Camera : Personal Finance',\n",
       "  'url': 'http://business-news.thestreet.com/ile-camera/story/can-you-protect-your-home-and-your-sanity-against-robo-calls/13208734',\n",
       "  'country': 'US',\n",
       "  'title': 'Can You Protect Your Home – and Your Sanity - Against Robo Calls?',\n",
       "  'performance_score': 0,\n",
       "  'site': 'thestreet.com',\n",
       "  'participants_count': 0,\n",
       "  'title_full': 'Can You Protect Your Home – and Your Sanity - Against Robo Calls?',\n",
       "  'spam_score': 0.0,\n",
       "  'site_type': 'news',\n",
       "  'published': '2015-07-06T19:30:00.000+03:00',\n",
       "  'replies_count': 0,\n",
       "  'uuid': 'cb34de284d3c1c2434da1c3fe59e85e692f2fbfb'},\n",
       " 'author': '',\n",
       " 'url': 'http://business-news.thestreet.com/ile-camera/story/can-you-protect-your-home-and-your-sanity-against-robo-calls/13208734',\n",
       " 'ord_in_thread': 0,\n",
       " 'title': 'Can You Protect Your Home – and Your Sanity - Against Robo Calls?',\n",
       " 'locations': [],\n",
       " 'entities': {'persons': [], 'locations': [], 'organizations': []},\n",
       " 'highlightText': '',\n",
       " 'language': 'english',\n",
       " 'persons': [\"Brian O'Connell\"],\n",
       " 'text': 'Can You Protect Your Home – and Your Sanity - Against Robo Calls? Written by: Brian O\\'Connell 07/06/15 - 12:30 PM EDT \\nNEW YORK ( MainStreet ) -- Americans do not like so-called \"robocalls\" - telemarketing calls from companies looking to pitch products and services, some of them with fraudulent intentions. According to Consumer Reports , U.S. consumers have placed 217 million numbers do-not-call requests since the agency began tracking them, while telemarketing fraud costs Americans $350 million annually. Still, companies manage to slip through the cracks and make robocalls, anyway. According to the Federal Trade Commission , the FTC receives between 250,000 and 300,000 consumer complaints per month against telemarketers, 60% of them directly linked to robocalls. Shel Horowitz, a green business profitability expert, is one of those consumers who have a big problem with robocallers. \"I especially get annoyed to see my own name and phone showing up as the call originator on my own caller ID, meaning they are probably poisoning the world against me by masquerading as me to others too,\" he says. Other consumers say it\\'s an uphill battle to block all robocallers, but there are some effective ways to keep annoying telemarketers at arm\\'s length. \"There really is no pro-active way to protect your home from robocalls,\" says Larry Peck, a consumer who is constantly battling robocallers. \"Most companies who do them ignore the Do-Not-Call list anyway, and I\\'ll call them back and get them to take me off their list.\" \"If they don\\'t, I block them via Comcast\\'s call blocking feature,\" he added. \"I can also block their number on my actual landline phone too. As far as my cell phone, Verizon lets me block up to five numbers on a rotating basis. But the problem is, you\\'re always dealing with robocalls on a reactive basis.\" Job one for consumers looking to block robocallers is to sign up for the federal Do-Not-Call List which will prevent legitimate telemarketers from calling you, says Steven J.J. Weisman, an attorney and consumer fraud specialist based in Amherst, Mass. \"Even if you are on the Do-Not-Call List, the law still permits you to receive robocalls from charities, politicians and polltakers,\" Weisman says. \"However, whenever you receive any call, particularly a robocall, you have no way of knowing who is really calling you. So the best thing to do is to not bother to pick up the phone if you don\\'t recognize the number - let legitimate callers leave a message and return their calls.\" Consumers can download some handy mobile apps that can steer them clear of aggressive telemarketers, advises Keeon Rudder, a motivational speaker and global business consultant. \"The easiest way to avoid spam calls is to download an app to hang up on such incoming calls,\" Rudder says. \"I\\'ve found Mr. Number to be quite effective in thwarting unwanted callers.\" Clair Jones, a home automation, Internet, and phone service expert with Localinternetservice.com, says the best thing you can do if you receive a robocall is to hang up immediately. \"If you speak or push any buttons on your phone you are inadvertently indicating interest and signaling that you are a good target for future calls,\" Jones says. \"To stop these annoying calls entirely, you can sign up for free services like Nomorobo, which automatically block unwanted robocalls while still allowing you to receive the ones that originate from trusted sources, like pharmacy refill reminders or Amber Alerts.\" More Headlines',\n",
       " 'external_links': [],\n",
       " 'published': '2015-07-06T19:30:00.000+03:00',\n",
       " 'crawled': '2015-07-06T21:06:03.000+03:00',\n",
       " 'highlightTitle': ''}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1303\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "PATH = '../../ConsensusIO/dependencies/meta'\n",
    "with open(PATH, 'rb') as fp:\n",
    "    data_raw = pickle.load(fp)\n",
    "X_test, y_test = data_raw[\"X\"], np.array(data_raw[\"y\"])\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1303x20000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 41115 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_transformed = text_clf.transform(X_test)\n",
    "X_test_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(694,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_yn = X_test_transformed[y_test!=1]\n",
    "y_test_yn = y_test[y_test!=1]\n",
    "y_test_yn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = [LDAPredict(article, model.topic_word_).index(max(LDAPredict(article, model.topic_word_))) for article in X_test_yn]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42939481268011526"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(pred,y_test_yn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDAPredict(val, topic_word):\n",
    "    return [sum(val*topic) for topic in topic_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
